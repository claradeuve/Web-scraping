{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4220fb3f",
   "metadata": {},
   "source": [
    "## Exercici 1\n",
    "Realitza web scraping d'una pàgina de la borsa de Madrid (https://www.bolsamadrid.es) utilitzant BeautifulSoup i Selenium."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7da2c9c",
   "metadata": {},
   "source": [
    "**BeautifulSoup**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcafd9e7",
   "metadata": {},
   "source": [
    "En primer lugar, inspecciono el contenido de la página web"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb8ba80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "URL = \"https://www.bolsamadrid.es/esp/aspx/Mercados/Precios.aspx?indice=ESI100000000&punto=indice\"\n",
    "page = requests.get(URL)\n",
    "\n",
    "print(page.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affa18f6",
   "metadata": {},
   "source": [
    "Decido la información que quiero obtener, el conjunto de diferencias de precios de las empresas del IBEX35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae4b2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "soup = BeautifulSoup(page.content, \"html.parser\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd26955",
   "metadata": {},
   "source": [
    "Localizo las clases HTML que me interesan y recojo la información. Aprovecho para comprobar que todos los datos que necesito están presentes y que no hay datos extra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "078afaf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "empresas = soup.find_all(\"td\", class_=\"DifFlBj\")\n",
    "difs = soup.find_all(\"td\", class_=\"DifClBj\")\n",
    "\n",
    "empresas_bj = []\n",
    "for empresa in empresas:\n",
    "    empresas_bj.append(empresa.text.strip())\n",
    "\n",
    "print(len(empresas_bj), empresas_bj)\n",
    "\n",
    "difs_bj = []\n",
    "for dif in difs:\n",
    "    difs_bj.append(dif.text.strip())\n",
    "    \n",
    "print(len(difs_bj), difs_bj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f7e154",
   "metadata": {},
   "outputs": [],
   "source": [
    "empresas = soup.find_all(\"td\", class_=\"DifFlSb\")\n",
    "difs = soup.find_all(\"td\", class_=\"DifClSb\")\n",
    "\n",
    "empresas_sb = []\n",
    "for empresa in empresas:\n",
    "    empresas_sb.append(empresa.text.strip())\n",
    "\n",
    "print(len(empresas_sb), empresas_sb)\n",
    "\n",
    "difs_sb = []\n",
    "for dif in difs:\n",
    "    difs_sb.append(dif.text.strip())\n",
    "    \n",
    "print(len(difs_sb), difs_sb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abde17c0",
   "metadata": {},
   "source": [
    "Compruebo que, junto a la información relativa a las empresas, se incluye información del IBEX35 en conjunto (el nombre, IBEX35 y dos precios relativos a la media anual y la de ese momento)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "015c16c2",
   "metadata": {},
   "source": [
    "Por tanto, elimino de la colección de datos la información general del IBEX35. Primero el nombre:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8a822d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Averiguar si el IBEX ha subido o ha bajado\n",
    "if \"IBEX 35®\" in empresas_sb:\n",
    "    print(empresas_sb)\n",
    "    index = empresas_sb.index(\"IBEX 35®\")\n",
    "    empresas_sb.remove(\"IBEX 35®\")\n",
    "    print(empresas_sb)\n",
    "elif \"IBEX 35®\" in empresas_bj:\n",
    "    print(empresas_bj)\n",
    "    index = empresas_bj.index(\"IBEX 35®\")\n",
    "    empresas_bj.remove(\"IBEX 35®\")\n",
    "    print(empresas_bj)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3acb03b",
   "metadata": {},
   "source": [
    " Y luego sus dos precios, tanto si indican incremento como disminución"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d668d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(empresas_sb), len(difs_sb))\n",
    "\n",
    "while len(empresas_sb) < len(difs_sb):\n",
    "    difs_sb.remove(f\"{difs_sb[0]}\")\n",
    "    \n",
    "print(len(empresas_sb), len(difs_sb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d125db5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(empresas_bj), len(difs_bj))\n",
    "\n",
    "while len(empresas_bj) < len(difs_bj):\n",
    "    index= difs_bj[0]\n",
    "    difs_bj.remove(f\"{difs_bj[0]}\")\n",
    "    \n",
    "print(len(empresas_bj), len(difs_bj))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba3d270",
   "metadata": {},
   "source": [
    "De esta manera, las longitudes de todas las listas de datos (empresas y precios) cuadran y puedo estudiar los datos y trabajar con ellos. Primero creo un dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ec9127",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_ibex = pd.DataFrame()\n",
    "\n",
    "df_ibex[\"Empresa\"] = empresas_bj + empresas_sb\n",
    "df_ibex[\"Dif\"] = difs_bj + difs_sb\n",
    "df_ibex = df_ibex.sort_values(by=['Empresa'])\n",
    "\n",
    "display(df_ibex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55634ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ibex.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e395e2a1",
   "metadata": {},
   "source": [
    "Convierto la columna Dif, los precios, a float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1528be79",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ibex['Dif'] = df_ibex['Dif'].str.replace(',','.')\n",
    "df_ibex['Dif'] = pd.to_numeric(df_ibex['Dif'])\n",
    "df_ibex.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f30286dd",
   "metadata": {},
   "source": [
    "Y visualizo los datos en un gráfico:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63207ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import seaborn\n",
    "\n",
    "a4_dims = (11.7, 8.27)\n",
    "fig, ax = plt.subplots(figsize=a4_dims)\n",
    "seaborn.barplot(x=\"Dif\", y=\"Empresa\", data=df_ibex)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1eb7d98",
   "metadata": {},
   "source": [
    "**Selenium**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a871796c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from collections import namedtuple\n",
    "from os.path import isfile\n",
    "from threading import Thread\n",
    "import csv\n",
    "from time import sleep\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "BOLSA_PRECIOSIBEX ='https://www.bolsamadrid.es/esp/aspx/Mercados/Precios.aspx?indice=ESI100000000&punto=indice'\n",
    "precios_sesion = namedtuple('precios_sesion', ['company', 'dif', 'timestamp'])\n",
    "\n",
    "\n",
    "class PreciosIbex():\n",
    "    def __init__(self):\n",
    "\n",
    "        options = webdriver.ChromeOptions()\n",
    "        options.add_argument(\"--start-maximized\")\n",
    "        options.add_argument('--log-level=3')\n",
    "        options.add_argument('--headless')\n",
    "\n",
    "        self.driver = webdriver.Chrome(\n",
    "            executable_path=\"chromedriver\",\n",
    "            options=options\n",
    "        )\n",
    "        self.driver.get(BOLSA_PRECIOSIBEX)\n",
    "\n",
    "        self.empresas_sb = []\n",
    "        self.empresas_bj = []\n",
    "        self.difs_sb = []\n",
    "        self.difs_bj = []\n",
    "        \n",
    "        self.empresas_all = []\n",
    "        self.difs_all = []\n",
    "        self.horas = []\n",
    "        \n",
    "        self.df_ibex = pd.DataFrame()\n",
    "        \n",
    "        self.datos()\n",
    "                \n",
    "        self.fecha = datetime.date(datetime.now())\n",
    "        self.database_path= f\"precios_IBEX35_{self.fecha}.csv\"\n",
    "        self.database = []\n",
    "        \n",
    "        if isfile(self.database_path):\n",
    "            with open(self.database_path, newline='') as dbfile:\n",
    "                dbreader = csv.reader(dbfile)\n",
    "                next(dbreader)\n",
    "        \n",
    "        self.thread = Thread(target=self.maintain)\n",
    "        self.thread.daemon = True \n",
    "        self.thread.start()\n",
    "        \n",
    "    \n",
    "    def maintain(self):\n",
    "        while True:\n",
    "            self.datos()\n",
    "            self.update_db()\n",
    "            sleep(20)\n",
    "    \n",
    "    \n",
    "    def save_db(self):\n",
    "        with open(self.database_path,'w', newline='') as dbfile:\n",
    "            dbwriter = csv.writer(dbfile)\n",
    "            dbwriter.writerow(list(precios_sesion._fields))\n",
    "            for entry in self.database:\n",
    "                dbwriter.writerow(list(entry))\n",
    "                \n",
    "\n",
    "    def update_db(self):\n",
    "        check = ((\n",
    "            (len(self.empresas_sb) > 0 and len(self.difs_sb) > 0) \n",
    "            or \n",
    "            (len(self.empresas_bj) > 0 and len(self.difs_bj) > 0)\n",
    "        ))\n",
    "\n",
    "        if check:\n",
    "            for i in range(len(self.empresas_all)):\n",
    "                self.database.append([self.empresas_all[i], self.difs_all[i], self.horas[i]])\n",
    "                self.save_db()\n",
    "\n",
    "        \n",
    "    def datos(self):\n",
    "        '''\n",
    "        Recabar información de la página relativa a cambios en el IBEX\n",
    "        '''\n",
    "        self.driver.get(BOLSA_PRECIOSIBEX)\n",
    "        empresas_sb = self.driver.find_elements(By.CLASS_NAME,'DifFlSb')\n",
    "        empresas_bj = self.driver.find_elements(By.CLASS_NAME,'DifFlBj')\n",
    "        difs_sb = self.driver.find_elements(By.CLASS_NAME,'DifClSb')\n",
    "        difs_bj = self.driver.find_elements(By.CLASS_NAME,'DifClBj')\n",
    "        horas = self.driver.find_elements(By.CLASS_NAME,'Ult')\n",
    "        \n",
    "        self.empresas_sb = [i.text for i in empresas_sb]\n",
    "        self.empresas_bj = [i.text for i in empresas_bj]\n",
    "        self.difs_sb = [i.text for i in difs_sb]\n",
    "        self.difs_bj = [i.text for i in difs_bj]\n",
    "        self.horas = [i.text for i in horas]\n",
    "        \n",
    "        self.eliminar_info_gral_IBEX()\n",
    "        print(self.horas)\n",
    "        self.convertir_float()\n",
    "        self.crear_tabla()\n",
    "    \n",
    "    \n",
    "    def eliminar_info_gral_IBEX(self):\n",
    "              \n",
    "        if \"IBEX 35®\" in self.empresas_sb:\n",
    "            self.empresas_sb.remove(\"IBEX 35®\")\n",
    "        elif \"IBEX 35®\" in self.empresas_bj:\n",
    "            self.empresas_bj.remove(\"IBEX 35®\")\n",
    "\n",
    "        while len(self.empresas_sb) < len(self.difs_sb):\n",
    "            self.difs_sb.remove(f\"{self.difs_sb[0]}\")\n",
    "        while len(self.empresas_bj) < len(self.difs_bj):\n",
    "            self.difs_bj.remove(f\"{self.difs_bj[0]}\")\n",
    "        \n",
    "        self.empresas_all = self.empresas_sb + self.empresas_bj        \n",
    "        self.difs_all = self.difs_sb + self.difs_bj\n",
    "        \n",
    "        while len(self.difs_all) < len(self.horas):\n",
    "            self.horas.remove(f\"{self.horas[0]}\")\n",
    "        \n",
    "        \n",
    "    def convertir_float(self):\n",
    "        floated_difs = []\n",
    "        for dif in self.difs_all:\n",
    "            dif = dif.replace(\",\", \".\")\n",
    "            dif = float(dif)\n",
    "            floated_difs.append(dif)\n",
    "        self.difs_all = floated_difs\n",
    "        \n",
    "        \n",
    "    def crear_tabla(self):\n",
    "        self.df_ibex[\"Empresa\"] = self.empresas_bj + self.empresas_sb\n",
    "        self.df_ibex[\"Dif\"] = self.difs_bj + self.difs_sb\n",
    "        self.df_ibex['Dif'] = self.df_ibex['Dif'].str.replace(',','.')\n",
    "        self.df_ibex['Dif'] = pd.to_numeric(self.df_ibex['Dif'])\n",
    "        self.df_ibex['Hora'] = self.horas\n",
    "        \n",
    "        \n",
    "    def ver_grafico(self):\n",
    "        self.crear_tabla()\n",
    "        a4_dims = (11.7, 8.27)\n",
    "        fig, ax = plt.subplots(figsize=a4_dims)\n",
    "        seaborn.barplot(x=\"Dif\", y=\"Empresa\", data=df_ibex)\n",
    "        \n",
    "    \n",
    "    def ver_datos(self):\n",
    "        self.crear_tabla()\n",
    "        display(self.df_ibex)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f444c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "precios_ibex = PreciosIbex()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44cf1c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "precios_ibex.ver_grafico()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d343f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "precios_ibex.ver_datos()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0449c241",
   "metadata": {},
   "source": [
    "**Scrapy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6268abe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "import json\n",
    "\n",
    "\"\"\"\n",
    "import sys\n",
    "from twisted.internet import default\n",
    "\n",
    "if 'twisted.internet.reactor' in sys.modules:\n",
    "    del sys.modules['twisted.internet.reactor']\n",
    "\n",
    "default.install()\n",
    "\"\"\"\n",
    "\n",
    "class JsonWriterPipeline(object):\n",
    "\n",
    "    def open_spider(self, spider):\n",
    "        self.file = open('birds_results.jl', 'w')\n",
    "\n",
    "    def close_spider(self, spider):\n",
    "        self.file.close()\n",
    "\n",
    "    def process_item(self, item, spider):\n",
    "        line = json.dumps(dict(item)) + \"\\n\"\n",
    "        self.file.write(line)\n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ab73ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from string import ascii_uppercase\n",
    "\n",
    "\n",
    "class BirdSpider(scrapy.Spider):\n",
    "    name = \"birds\"\n",
    "    start_urls = [\n",
    "        f'https://seo.org/listado-aves-2/?letra={letter}' for letter in ascii_uppercase\n",
    "    ]\n",
    "    custom_settings = {\n",
    "        'LOG_LEVEL': logging.WARNING,\n",
    "        'ITEM_PIPELINES': {'__main__.JsonWriterPipeline': 1}, # Used for pipeline 1\n",
    "        'FEEDS':{'birds_results.csv':{'format':'csv'}}\n",
    "    }\n",
    "    \n",
    "    def parse(self, response):\n",
    "        for bird in response.css('div.contenedor_txt'):\n",
    "            yield {\n",
    "                'namme_common': bird.css('h3 a::text').extract_first(),\n",
    "                'name_scientific': bird.css('p::text').extract_first(),\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4a2c19f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-04 19:43:57 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: scrapybot)\n",
      "2022-04-04 19:43:57 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.2.0, Python 3.9.7 (default, Sep 16 2021, 08:50:36) - [Clang 10.0.0 ], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform macOS-10.16-x86_64-i386-64bit\n",
      "2022-04-04 19:43:57 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'LOG_LEVEL': 30,\n",
      " 'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)'}\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from twisted.internet import default\n",
    "\n",
    "# Permite ejecutar este mismo bloque de código sin que genere \"reactoralreadyinstalled.error\"\n",
    "if 'twisted.internet.reactor' in sys.modules:\n",
    "    del sys.modules['twisted.internet.reactor']\n",
    "\n",
    "    \n",
    "process = CrawlerProcess({\n",
    "    'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)'\n",
    "})\n",
    "\n",
    "process.crawl(BirdSpider)\n",
    "process.start()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
